function [agent, trainingStats] = train_agent(model)
%TRAIN_AGENT Build env, networks, and train DDPG agent

rng(0); Ts = 1; Tf = 500;

% Specs
obsInfo = rlNumericSpec([3 1], 'LowerLimit', [-inf -inf -inf]', 'UpperLimit', [inf inf inf]');
actInfo = rlNumericSpec([3 1]);

% Env
env = make_env(model, [model '/RL Agent'], obsInfo, actInfo);
env.ResetFcn = @(in)localResetFcn(in);

% Networks
critic = build_critic(obsInfo, actInfo);
actor  = build_actor(obsInfo, actInfo);

% Agent options
agentOpts = rlDDPGAgentOptions( ...
    'SampleTime', Ts, ...
    'TargetSmoothFactor', 1e-3, ...
    'DiscountFactor', 1.0, ...
    'MiniBatchSize', 64, ...
    'ExperienceBufferLength', 1e6);
agentOpts.NoiseOptions.Variance = 0.3;
agentOpts.NoiseOptions.VarianceDecayRate = 1e-5;

agent = rlDDPGAgent(actor, critic, agentOpts);

% Training options
maxepisodes = 5000; maxsteps = ceil(Tf/Ts);
trainOpts = rlTrainingOptions( ...
    'MaxEpisodes', maxepisodes, ...
    'MaxStepsPerEpisode', maxsteps, ...
    'ScoreAveragingWindowLength', 5, ...
    'Verbose', false, ...
    'Plots', 'training-progress', ...
    'StopTrainingCriteria', 'AverageReward', ...
    'StopTrainingValue', 998);

% Train
trainingStats = train(agent, env, trainOpts);
end
